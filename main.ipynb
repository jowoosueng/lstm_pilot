{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today:  230310 \n",
      "Current path:  c:\\Joko\\Project\\Bigdata\\lstm_pilot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gym\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "current_path = os.getcwd() # path of folder\n",
    "today = datetime.today().strftime(\"%y%m%d\")\n",
    "print(\"Today: \", today, \"\\nCurrent path: \", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed = 28947\n",
    "set_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "- Progress of code\n",
    "\n",
    "1. prepare data for training\n",
    "2. define lstm network architecture\n",
    "3. specify training options\n",
    "4. train neural network\n",
    "5. test network\n",
    "6. forcast future time step\n",
    "   (1) open loop forecasting\n",
    "   (2) close loop forecasting \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pilot test 하려고 원래 데이터 source에서 data 폴더로 데이터 옮기는 코드 (랜덤하게 넣기위해)\n",
    "\"\"\"\n",
    "np.random.seed(seed) # 왠진 모르겠는데 seed가 안먹힘. 그냥하자 일단. 이거 할 때 여러번 누르면 데이터 갯수가 늘어나니깐 조심\n",
    "source_lst = [\"Train\", \"Valid\", \"Test\"]\n",
    "# source = source_lst[0]\n",
    "for source in source_lst:\n",
    "    source_path = current_path + '/source/' + source + '/'\n",
    "    saved_path = current_path + '/data/' + source + '/'\n",
    "\n",
    "    file_lst = os.listdir(source_path)\n",
    "    sample_lst = random.sample(file_lst, int(len(file_lst)/10))\n",
    "\n",
    "    for sample in sample_lst:\n",
    "        csv = pd.read_csv(source_path + sample)\n",
    "        df = pd.DataFrame(csv)\n",
    "        df.to_csv(saved_path + sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 데이터셋\n",
    "Train: 188개\n",
    "Valid: 39개\n",
    "Test: 40개\n",
    "\n",
    "데이터는 하던 Delaware data에서 10%씩 빼온 거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_rate_lst = df['Gas_rate'].values\n",
    "cumgas_rate_lst = []\n",
    "for idx, gas_rate in enumerate(gas_rate_lst):\n",
    "    if idx == 0:\n",
    "        cumgas_rate_lst = np.append(cumgas_rate_lst, gas_rate)\n",
    "    else:\n",
    "        cumgas = cumgas_rate_lst[idx-1] + gas_rate\n",
    "        cumgas_rate_lst = np.append(cumgas_rate_lst, cumgas)\n",
    "df['Cumgas_rate'] = cumgas_rate_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data normalization part\n",
    "Train data를 모두 불러와서 standard normalization을 수행\n",
    "\"\"\"\n",
    "train_path = current_path + '/data/Train/'\n",
    "train_lst = os.listdir(train_path)\n",
    "valid_path = current_path + '/data/Valid/'\n",
    "valid_lst = os.listdir(valid_path)\n",
    "\n",
    "input_features = ['Gas_rate', 'Cumgas_rate', 'TotalProdMonths', 'ShutinMonths', 'Refrac']\n",
    "chcek_scaling_point = 3 # input_features 중에서 어디까지 normalization할 것인지 수기로 정해줘야함. (파이썬 인덱스 생각말고, 그냥 그 위치를 넣어주면 됨)\n",
    "\n",
    "scaler = StandardScaler() ## ShutinMonths, Refrac은 Scaling 하지 않음. 이미 0 or 1인 데이터임. 나머지는 standard_normalization\n",
    "\n",
    "df_tot_tmp = pd.DataFrame()\n",
    "df_tot = pd.DataFrame(columns=input_features)\n",
    "for file in train_lst:\n",
    "    df_tmp = pd.read_csv(train_path + file)\n",
    "\n",
    "    gas_rate_lst = df_tmp['Gas_rate'].values ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "    cumgas_rate_lst = []\n",
    "    for idx, gas_rate in enumerate(gas_rate_lst):\n",
    "        if idx == 0:\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, gas_rate)\n",
    "        else:\n",
    "            cumgas = cumgas_rate_lst[idx-1] + gas_rate\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, cumgas)\n",
    "    df_tmp['Cumgas_rate'] = cumgas_rate_lst ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "\n",
    "    df_tot_tmp = pd.concat([df_tot_tmp, df_tmp])\n",
    "\n",
    "df_tot['Gas_rate'] = df_tot_tmp['Gas_rate']\n",
    "for x in input_features:\n",
    "    df_tot[x] = df_tot_tmp[x] ## 이게 없으면 열 순서가 자동으로 바뀜\n",
    "\n",
    "scaler.fit(df_tot.values[:, :chcek_scaling_point]) ############ 조금 많이 헷갈릴 수 있는데, scaling 자체를 안하는 애들이 있어서 했다가 더하는 식으로 데이터를 구성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows_mutli_features(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range((data.shape[0])-seq_length):\n",
    "        _x = data[i:(i+seq_length), :]\n",
    "        # _x_prod = data[i:(i+seq_length), 0:1] ## 3 columns for features\n",
    "        # _x_oper = data[i+1:(i+seq_length+1), 1:]\n",
    "        # _x = np.concatenate((_x_prod, _x_oper), axis=1) ## 이부분은 cascading 식으로 예측할 때, 시점이 서로 달라야할 때 필요\n",
    "        _y = data[i+seq_length, 0] ## column 0 contains the labbel\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# file = train_lst[0]\n",
    "for file in train_lst:\n",
    "    csv = pd.read_csv(train_path + file)\n",
    "    df = pd.DataFrame(csv)\n",
    "\n",
    "    gas_rate_lst = df['Gas_rate'].values ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "    cumgas_rate_lst = []\n",
    "    for idx, gas_rate in enumerate(gas_rate_lst):\n",
    "        if idx == 0:\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, gas_rate)\n",
    "        else:\n",
    "            cumgas = cumgas_rate_lst[idx-1] + gas_rate\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, cumgas)\n",
    "    df['Cumgas_rate'] = cumgas_rate_lst ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "    data = df[input_features].values\n",
    "    data_no_need_scaling = data[:, chcek_scaling_point:]\n",
    "    data_need_scaling = scaler.transform(data[:, :chcek_scaling_point])\n",
    "    data_scaled = np.concatenate((data_need_scaling, data_no_need_scaling), axis=1)\n",
    "    x, y = sliding_windows_mutli_features(data_scaled, seq_length)\n",
    "\n",
    "    X_train = np.concatenate((X_train, x), axis=None)\n",
    "    Y_train = np.concatenate((Y_train, y), axis=None)\n",
    "\n",
    "X_train = np.reshape(X_train, (-1, seq_length, len(input_features)))\n",
    "Y_train = np.reshape(Y_train, (-1, 1))\n",
    "\n",
    "################### Train과 똑같은 과정을 valid에서도, 단 scaling만 Train해서 한 값으로 함\n",
    "\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "\n",
    "# file = valid_lst[0]\n",
    "for file in valid_lst:\n",
    "    csv = pd.read_csv(valid_path + file)\n",
    "    df = pd.DataFrame(csv)\n",
    "\n",
    "    gas_rate_lst = df['Gas_rate'].values ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "    cumgas_rate_lst = []\n",
    "    for idx, gas_rate in enumerate(gas_rate_lst):\n",
    "        if idx == 0:\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, gas_rate)\n",
    "        else:\n",
    "            cumgas = cumgas_rate_lst[idx-1] + gas_rate\n",
    "            cumgas_rate_lst = np.append(cumgas_rate_lst, cumgas)\n",
    "    df['Cumgas_rate'] = cumgas_rate_lst ############### 전처리를 잘못해서 cumgas rate 생성 코드가 중간에 들어가야함\n",
    "    data = df[input_features].values\n",
    "    data_no_need_scaling = data[:, chcek_scaling_point:]\n",
    "    data_need_scaling = scaler.transform(data[:, :chcek_scaling_point])\n",
    "    data_scaled = np.concatenate((data_need_scaling, data_no_need_scaling), axis=1)\n",
    "    x, y = sliding_windows_mutli_features(data_scaled, seq_length)\n",
    "\n",
    "    X_valid = np.concatenate((X_valid, x), axis=None)\n",
    "    Y_valid = np.concatenate((Y_valid, y), axis=None)\n",
    "\n",
    "X_valid = np.reshape(X_valid, (-1, seq_length, len(input_features)))\n",
    "Y_valid = np.reshape(Y_valid, (-1, 1))\n",
    "\n",
    "validX = Variable(torch.Tensor(X_valid))\n",
    "validY = Variable(torch.Tensor(Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = Variable(torch.Tensor(self.x_data[idx]))\n",
    "        y = Variable(torch.Tensor(self.y_data[idx]))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X_train, Y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "현재 데이터셋 구축까지 완료함.\n",
    "\n",
    "이제 모델 구축하고, 학습한 다음에\n",
    "Train / Valid 성능 확인하는 부분만 남음. 최종 수정일시 : 230310 오후 8시 08분 \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
